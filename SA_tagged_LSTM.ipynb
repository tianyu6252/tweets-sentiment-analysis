{"cells":[{"cell_type":"markdown","metadata":{"id":"ZkMgzCGMwFMW"},"source":["# LSTM TRAIN CODE"]},{"cell_type":"markdown","metadata":{"id":"kgPU06rDwFMb"},"source":["## Import Libraries"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"X9LX1gxfwFMc"},"outputs":[],"source":["import pandas as pd\n","pd.set_option('display.max_colwidth', None)\n","# pd.set_option('display.max_rows', None)\n","import numpy as np\n","from numpy import load\n","import pickle\n","\n","import tensorflow as tf\n","import keras\n","from keras.preprocessing import text, sequence\n","from keras.preprocessing.sequence import pad_sequences\n","from keras.preprocessing.text import Tokenizer\n","from keras.models import Sequential\n","from keras.layers import Dense, Embedding, InputLayer, LSTM, GRU, Conv1D, MaxPooling1D, Flatten, Dropout, Activation, GlobalMaxPool1D\n","from keras import regularizers\n","from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n","from tensorflow.keras.optimizers import Adam\n","from keras import backend as K\n","from keras.utils import np_utils\n","\n","from sklearn.model_selection import train_test_split, KFold, cross_val_score, GridSearchCV\n","from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, HashingVectorizer\n","from sklearn.feature_selection import SelectFromModel\n","from sklearn.linear_model import LogisticRegression\n","\n","import torch # pytorch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torchtext.vocab\n","from torchtext.legacy import data\n","from torchtext.legacy import datasets\n","\n","import gensim\n","from gensim.models import Word2Vec, KeyedVectors\n","\n","import re\n","import string\n","from string import punctuation\n","import seaborn as sb\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","import time\n","import statistics\n","\n","# Open-source Sentiment Analysis libraries\n","from textblob import TextBlob\n","import nltk\n","from nltk.sentiment import SentimentIntensityAnalyzer\n","nltk.download('vader_lexicon')\n","\n","# !pip install emoji\n","# from preprocessing_steps import Preprocess\n","\n"]},{"cell_type":"markdown","metadata":{"id":"BIblGwYXPFaZ"},"source":["## Load Google drive"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"03vYyyNgsepM"},"outputs":[],"source":["# For access to Gdrive to write result charts into it\n","from google.colab import auth, files, drive\n","drive.mount('/content/gdrive')\n","%cd /content/gdrive/MyDrive/Colab_Project/tweet_sentiment_analysis"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NmG5qC_8wFMd"},"outputs":[],"source":["is_cuda = torch.cuda.is_available()\n","if is_cuda:\n","    device = torch.device(\"cuda\")\n","    print(\"GPU is available\")\n","else:\n","    device = torch.device(\"cpu\")\n","    print(\"GPU not available, CPU used\") "]},{"cell_type":"markdown","metadata":{"id":"PvxY9-q_wFMe"},"source":["## Load preprocessed data for lstm model (2020_09_2021_02)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rfoAji_qwFMe"},"outputs":[],"source":["file_path = \"data1/emojitag_remove_finalized_tweets_training_data_2020_09_to_2021_02.csv\"\n","data_df = pd.read_csv(file_path)\n","data_df = data_df.dropna(subset=[\"full_text\", \"processed_text\"])\n","data_copy = data_df.copy()\n","print(len(data_df))\n","\n","data_df = data_df[[\"full_text\", \"processed_text\", \"sentiment_Label_AVG\"]] # Remove unnecessary columns\n","data_df[\"processed_text\"] = data_df[\"processed_text\"].astype(str) # Ensure column is string type\n","data_df.sample(n=2)"]},{"cell_type":"markdown","metadata":{"id":"d_3O52jzym-P"},"source":["## Generate ground truth using textblob and vader"]},{"cell_type":"markdown","metadata":{"id":"psFgH5Qz7bxB"},"source":["### TextBlob"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"q5YsDBph7Oss"},"outputs":[],"source":["# start = time.time()\n","# sentiment_TextBlob = []\n","# for i in range(len(data_df)):\n","#     blob = TextBlob(data_df.iloc[i][\"full_text\"]) # Note: On processed_text\n","#     sentiment = \"neutral\"\n","#     if blob.sentiment.polarity == 0:\n","#         sentiment = \"neutral\"\n","#     elif blob.sentiment.polarity > 0:\n","#         sentiment = \"positive\"\n","#     elif blob.sentiment.polarity < 0:\n","#         sentiment = \"negative\"\n","        \n","#     sentiment_TextBlob.append(sentiment)\n","\n","# # Add in new column called \"sentiment_TextBlob\" with the sentiment value from TextBlob\n","# data_df[\"sentiment_TextBlob\"] = sentiment_TextBlob\n","# print(str(len(data_df)) + \" records\")\n","# print(\"\\nTotal time taken in minutes: {:.4f}\".format((time.time()-start) / 60))"]},{"cell_type":"markdown","metadata":{"id":"hF3EF4ec7i76"},"source":["### Vader"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"c2M1Hm2Q7iJQ"},"outputs":[],"source":["# sia = SentimentIntensityAnalyzer()\n","\n","# start = time.time()\n","# sentiment_Vader = []\n","# for i in range(len(data_df)):\n","#     # scores_dict = sia.polarity_scores(data_copy.iloc[i][\"text\"]) # Note: On unprocessed text because it has an exhaustive inbuilt preprocessing\n","#     scores_dict = sia.polarity_scores(data_df.iloc[i][\"full_text\"]) # Note: On unprocessed text because it has an exhaustive inbuilt preprocessing\n","#     sentiment = \"neutral\"\n","#     if scores_dict[\"compound\"] == 0:\n","#         sentiment = \"neutral\"\n","#     elif scores_dict[\"compound\"] > 0:\n","#         sentiment = \"positive\"\n","#     elif scores_dict[\"compound\"] < 0:\n","#         sentiment = \"negative\"\n","        \n","#     sentiment_Vader.append(sentiment)\n","\n","# # Add in new column called \"sentiment_TewxtBlob\" with the sentiment value from TextBlob\n","# data_df[\"sentiment_Vader\"] = sentiment_Vader\n","# print(str(len(data_df)) + \" records\")\n","# print(\"\\nTotal time taken in minutes: {:.4f}\".format((time.time()-start) / 60))"]},{"cell_type":"markdown","metadata":{"id":"so4nX7Mo7uuD"},"source":["### Textblob + Vader"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Hz6NE8NGzK8f"},"outputs":[],"source":["# raw_df = pd.read_csv(\"data1/emojitag_remove_finalized_tweets_training_data_2020_09_to_2021_02.csv\")\n","\n","# start = time.time()\n","# sia = SentimentIntensityAnalyzer()\n","# sentiment_TV = []\n","# for i in range(len(raw_df)):\n","#     blob = TextBlob(raw_df.iloc[i][\"full_text\"]) # Note: On processed_text\n","#     scores_dict = sia.polarity_scores(raw_df.iloc[i][\"full_text\"]) # Note: On unprocessed text because it has an exhaustive inbuilt preprocessing\n","#     sentiment = \"neutral\"\n","#     average_score = (blob.sentiment.polarity + scores_dict[\"compound\"])/2\n","#     # print(average_score)\n","#     if average_score > 0.1:\n","#         sentiment = \"positive\"\n","#     elif average_score < -0.1:\n","#         sentiment = \"negative\"\n","#     else: \n","#         sentimet = \"neutral\"\n","#     sentiment_TV.append(sentiment)\n","# raw_df[\"sentiment_TV\"] = sentiment_TV\n","\n","# print(str(len(raw_df)) + \" records\")\n","# print(\"\\nTotal time taken in minutes: {:.4f}\".format((time.time()-start) / 60))\n","# raw_df.sample(n=5)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-EkH27lF4If6"},"outputs":[],"source":["# raw_df.to_csv(\"data1/emojitag_remove_finalized_tweets_training_data_2020_09_to_2021_02.csv\", index=False)"]},{"cell_type":"markdown","metadata":{"id":"CkEz0ELawFMf"},"source":["## Check Ground Truth Label Distribution"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XcTTDFs8wFMf"},"outputs":[],"source":["# sb.set(style='whitegrid')\n","# sb.countplot(x='sentiment_Label_AVG', data=data_df) # Balanced data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QUvaEAqX77Vv"},"outputs":[],"source":["# sb.set(style='whitegrid')\n","# sb.countplot(x='sentiment_Vader', data=data_df) # Balanced data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sX9Zq4QI79Q4"},"outputs":[],"source":["# sb.set(style='whitegrid')\n","# sb.countplot(x='sentiment_TextBlob', data=data_df) # Balanced data"]},{"cell_type":"markdown","metadata":{"id":"B4pPhKavwFMj"},"source":["## Split data into train and test data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"g_UNj85TwFMk"},"outputs":[],"source":["train_df, test_df = train_test_split(data_df, test_size=0.2, random_state=0)\n","print(len(train_df))\n","train_df.head()"]},{"cell_type":"markdown","metadata":{"id":"KRs3nmOZwFMk"},"source":["# Word Embeddings"]},{"cell_type":"markdown","metadata":{"id":"y9D1dlOJwFMl"},"source":["## Use Self-Trained Word2Vec embedding model"]},{"cell_type":"markdown","metadata":{"id":"LfYLBKrrMVX9"},"source":["### Using 2020_06_2021_02 to generate word2vec model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"quCIaxJLwFMl"},"outputs":[],"source":["embedding_train_df = pd.read_csv('data/emojitag_remove_filtered_tweets_filtered_compiled_2020_06_to_2021_02.csv', encoding='utf-8')\n","embedding_train_df = embedding_train_df.dropna(subset=[\"full_text\", \"processed_text\"])\n","print(len(embedding_train_df))\n","embedding_train_df[[\"id\", \"full_text\", \"processed_text\"]] # Note: No labels. Keep id for cross-referencing\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wozl9JRewFMm"},"outputs":[],"source":["# Train word2Vec model for LSTM embedding layer \n","EMBED_DIM = 100\n","WORD2VEC_WINDOW = 7\n","WORD2VEC_EPOCH = 50\n","WORD2VEC_MIN_COUNT = 10\n","\n","doc = [txt.split() for txt in embedding_train_df[\"processed_text\"]] \n","# for txt in embedding_train_df[\"processed_text\"]:\n","#    if isinstance(txt, float): print(txt)\n","\n","word2vec_model = gensim.models.word2vec.Word2Vec(size=EMBED_DIM,\n","                                                 window=WORD2VEC_WINDOW,\n","                                                 min_count=WORD2VEC_MIN_COUNT,\n","                                                 workers=8)\n","\n","word2vec_model.build_vocab(doc)\n","words = word2vec_model.wv.vocab.keys()\n","print(\"Total num of words in vocab: \", len(words))\n","\n","start = time.time()\n","word2vec_model.train(doc, total_examples=len(doc), epochs=WORD2VEC_EPOCH)\n","print(\"\\nTotal time taken in minutes: {:.4f}\".format((time.time()-start) / 60))\n","\n","# Save word2vec model\n","# filename = \"lstm_word2vec_model_from_tweets_\" + str(EMBED_DIM) + \"_\" + str(WORD2VEC_WINDOW) + \"_\" + str(WORD2VEC_EPOCH) + \"_\" + str(WORD2VEC_MIN_COUNT) + \".w2v\"\n","# word2vec_model.save(filename)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mEhpKYdNL3DF"},"outputs":[],"source":["# print(word2vec_model.wv[''])\n","print(word2vec_model.wv.most_similar('covid', topn=10))"]},{"cell_type":"markdown","metadata":{"id":"kbbblE7ewFMm"},"source":["# Tokenize text content \n","- Tokenize the text content, then convert to a padded sequence to fit into LSTM model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5QN3pjzwwFMn"},"outputs":[],"source":["# deal with the train data\n","\n","tokenizer = Tokenizer(filters='')\n","tokenizer.fit_on_texts(train_df[\"processed_text\"])\n","print(tokenizer)\n","VOCAB_SIZE = len(tokenizer.word_index) + 1 # Note: index starts from 1 instead of 0. Therefore increase size by 1\n","print(\"VOCAB_SIZE:\", VOCAB_SIZE)\n","\n","# save tokenizer\n","tokenizer_file_path = \"remove_emojitag_weights/lstm_tokenizer.pkl\"\n","pickle.dump(tokenizer, open(tokenizer_file_path, \"wb\"), protocol=0)\n","tokenizer.word_index.items()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7zQlwD8BwFMn"},"outputs":[],"source":["SEQUENCE_LENGTH = 100\n","x_train = pad_sequences(tokenizer.texts_to_sequences(train_df[\"processed_text\"]), maxlen=SEQUENCE_LENGTH)\n","x_test = pad_sequences(tokenizer.texts_to_sequences(test_df[\"processed_text\"]), maxlen=SEQUENCE_LENGTH)\n","# len(x_train[1])"]},{"cell_type":"markdown","metadata":{"id":"6yauDwu4Kaqa"},"source":["## Preprocess labels"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"25p6FzMeKdmJ"},"outputs":[],"source":["# def convert_label(list):\n","#     labels = []\n","#     for label in list:\n","#        if label == 'positive': labels.append(1)\n","#        elif label == 'neutral': labels.append(0)\n","#        elif label == 'negative': labels.append(-1)\n","#     return labels\n","\n","encoder = LabelEncoder()\n","encoder.fit(train_df[\"sentiment_Label_AVG\"].tolist())\n","print(train_df[\"sentiment_Label_AVG\"].head())\n","\n","y_train = encoder.transform(train_df[\"sentiment_Label_AVG\"].tolist())\n","y_test = encoder.transform(test_df[\"sentiment_Label_AVG\"].tolist())\n","# y_train = np.array(convert_label(train_df[\"sentiment_TV\"].tolist()))\n","# y_test = np.array(convert_label(test_df[\"sentiment_TV\"].tolist()))\n","\n","y_train_num = y_train.reshape(-1, 1)\n","y_test_num = y_test.reshape(-1, 1)\n","print(y_train_num[:5])\n","\n","y_train = np_utils.to_categorical(y_train_num)\n","y_test = np_utils.to_categorical(y_test_num)\n","print(y_train[:5])  # Note: 'positive': 2->[0 0 1] 'neutral': 1->[0 1 0]      'negative': 0->[1 0 0]\n","\n","\n","print(\"x_train shape\", x_train.shape)\n","print(\"x_test shape\", x_test.shape)\n","print(\"y_train shape\", y_train.shape)\n","print(\"y_test shape\", y_test.shape)\n","print()\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"_7j4c0vGwFMo"},"source":["# Build and compile LSTM model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iYjSUq0aUEQN"},"outputs":[],"source":["embedding_matrix_file_path = \"remove_emojitag_weights/lstm_word2vec_embedding_matrix.npy\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WbEZEWaWwFMo"},"outputs":[],"source":["# Create embedding matrix for the embedding layer\n","EMBEDDING_MATRIX = np.zeros((VOCAB_SIZE, EMBED_DIM))\n","\n","# Self-trained word2Vec embedding\n","# convert each token in the train vocabulary to word vector\n","for word, index in tokenizer.word_index.items():\n","    if word in word2vec_model.wv.vocab.keys():\n","        EMBEDDING_MATRIX[index] = word2vec_model.wv[word]\n","        \n","print(EMBEDDING_MATRIX.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"x_dS8akVjLIR"},"outputs":[],"source":["# Save EMBEDDING_MATRIX(numpy array) as npy file\n","from numpy import asarray\n","from numpy import save\n","\n","save(embedding_matrix_file_path, EMBEDDING_MATRIX)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"cnMuHKORRMAv"},"source":["## Load embedding matrix and tokenizer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vDl0UqyaRRSq"},"outputs":[],"source":["# Load EMBEDDING_MATRIX(numpy array) from npy file\n","from numpy import load\n","\n","EMBEDDING_MATRIX = load(embedding_matrix_file_path)\n","print(\"EMBEDDING_MATRIX SHAPE:\", EMBEDDING_MATRIX.shape)\n","\n","VOCAB_SIZE = EMBEDDING_MATRIX.shape[0]\n","EMBED_DIM = EMBEDDING_MATRIX.shape[1]\n","\n","print(\"VOCAB_SIZE - Embedding Matrix:\", VOCAB_SIZE)\n","print(\"EMBED_DIM: \", EMBED_DIM)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QXaslYkAwFMo"},"outputs":[],"source":["\n","def get_LSTM_Model():\n","    model = Sequential()\n","    model.add(InputLayer(input_shape=(SEQUENCE_LENGTH,), dtype='int32'))\n","    \n","    # Non-trainable embedding layer\n","    model.add(\n","        Embedding(\n","            input_dim = VOCAB_SIZE,\n","            output_dim = EMBED_DIM,\n","            input_length = SEQUENCE_LENGTH,\n","            weights = [EMBEDDING_MATRIX],\n","            trainable=False\n","        ))\n","    \n","    # LSTM layer\n","    model.add(LSTM(128, return_sequences=True))\n","    model.add(GlobalMaxPool1D())\n","    model.add(Dropout(0.1))\n","    model.add(Dense(64, activation='relu'))\n","    model.add(Dropout(0.1))\n","    model.add(Dense(32, activation='relu'))\n","    model.add(Dropout(0.1))\n","    # model.add(Dense(1, activation='sigmoid'))\n","    model.add(Dense(3, activation='softmax'))\n","\n","    model.compile(loss='categorical_crossentropy',\n","                  optimizer='adam',\n","                  metrics=['accuracy'])\n","    \n","    return model"]},{"cell_type":"markdown","metadata":{"id":"wS-OaKEwwFMo"},"source":["# Train LSTM model"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":171770,"status":"ok","timestamp":1618125373051,"user":{"displayName":"Tianyu Fu","photoUrl":"","userId":"03175330516896294618"},"user_tz":-480},"id":"MwUoGaaAwFMo","outputId":"bda63164-8092-4927-a13a-2cc9eb8487f9"},"outputs":[],"source":["# Model params \n","BATCH_SIZE = 1024\n","EPOCHS = 30\n","\n","# Compile and get model\n","model_lstm = get_LSTM_Model()\n","print(model_lstm.summary())\n","print(\"\\n\")\n","\n","# Callbacks\n","reduce_lr = ReduceLROnPlateau(monitor='val_loss', patience=5, cooldown=0)\n","early_stop = EarlyStopping(monitor='val_accuracy', min_delta=0, patience=5) # To reduce overfitting\n","callbacks = [reduce_lr, early_stop]\n","\n","# Train model\n","start = time.time()\n","history = model_lstm.fit(x_train,\n","                         y_train,\n","                         batch_size=BATCH_SIZE,\n","                         epochs=EPOCHS,\n","                         verbose=1,\n","                         validation_split=0.1,\n","                         callbacks=callbacks)\n","\n","# Get test accuracy\n","print('\\nTest')\n","loss, accuracy = model_lstm.evaluate(x_test, y_test, batch_size=BATCH_SIZE, verbose=1)\n","print(\"\\nTest Accuracy = {}\".format(accuracy))\n","\n","print(\"\\nTotal time taken in mins: {:.4f}\".format((time.time()-start) / 60))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":285},"executionInfo":{"elapsed":172498,"status":"ok","timestamp":1618125373791,"user":{"displayName":"Tianyu Fu","photoUrl":"","userId":"03175330516896294618"},"user_tz":-480},"id":"cU2W8rRVwFMp","outputId":"13710973-bbd4-4af0-bc24-07758f74f2c7"},"outputs":[],"source":["# Plot Train vs Validation accuracy\n","sb.set(font_scale=1.0) # Determine fontsize\n","train_acc = history.history['accuracy'] # Blue\n","val_acc = history.history['val_accuracy'] # Orange\n","plt.plot(train_acc, 'C0', label=\"Train Accuracy\")\n","plt.plot(val_acc, 'C1', label=\"Validation Accuracy\")\n","plt.legend(loc=\"upper left\")\n","plt.xlabel(\"No. of epochs\")\n","plt.ylabel(\"Accuracy Pct\")\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"sNEEBlBQwFMp"},"source":["# Evaluate test data"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":173450,"status":"ok","timestamp":1618125374751,"user":{"displayName":"Tianyu Fu","photoUrl":"","userId":"03175330516896294618"},"user_tz":-480},"id":"T2H_lvZdwFMp","outputId":"e380623c-c7d0-4155-9946-e76aa40e9d58"},"outputs":[],"source":["# represent by number pos: 2, neu: 1, neg:0\n","y_pred = model_lstm.predict_classes(x_test, verbose=0)\n","same = 0\n","for i, y_true in enumerate(y_test_num):\n","    if y_true == y_pred[i]:\n","       same = same + 1\n","print(len(y_pred) == len(y_test_num))\n","ac_check = same / len(y_test_num)\n","ac_check\n","\n","accuracy = accuracy_score(y_test_num, y_pred)\n","print(\"Test Accuracy = {}\".format(accuracy))\n","\n","f1 = f1_score(y_test_num, y_pred, average=\"macro\")\n","print(\"Test F1 macro score = {}\".format(f1))\n","# print(classification_report(y_test, y_pred, target_names = ['negative', 'positive']))"]},{"cell_type":"markdown","metadata":{"id":"cAleaR-JwFMp"},"source":["# Perform Sample Predictions"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9gkCIBB1wFMr"},"outputs":[],"source":["# # Print confusion matrix\n","# cm = confusion_matrix(y_test, y_pred)\n","# cm = pd.DataFrame(cm, index=['negative','positive'], columns=['negative','positive'])\n","\n","# sb.set(font_scale=2.0) # Increase fontsize\n","# plt.figure(figsize = (10,10))\n","# sb.heatmap(cm, cmap=\"Blues\", linecolor='black', linewidth=1, annot=True, fmt='')\n","# plt.xlabel(\"Predicted\")\n","# # plt.ylabel(\"Actual\")"]},{"cell_type":"markdown","metadata":{"id":"DNsfpuYzwFMr"},"source":["# Save models"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f6sW-JlOwFMr"},"outputs":[],"source":["file_path = \"remove_emojitag_weights/lstm_main_model.h5\"\n","model_lstm.save(file_path)\n","\n","# # EXPORT\n","# LSTM_MODEL = \"model_lstm.h5\"\n","# WORD2VEC_MODEL = \"model.w2v\"\n","# TOKENIZER_MODEL = \"tokenizer.pkl\"\n","# ENCODER_MODEL = \"encoder.pkl\"\n","\n","# model_lstm.save(LSTM_MODEL)\n","# word2vec_model.save(WORD2VEC_MODEL)\n","# pickle.dump(tokenizer, open(TOKENIZER_MODEL, \"wb\"), protocol=0)\n","# pickle.dump(encoder, open(ENCODER_MODEL, \"wb\"), protocol=0)"]},{"cell_type":"markdown","metadata":{"id":"Cp8mFRLfhlTD"},"source":["# Test on manually labeled data"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":553},"executionInfo":{"elapsed":173991,"status":"ok","timestamp":1618125375310,"user":{"displayName":"Tianyu Fu","photoUrl":"","userId":"03175330516896294618"},"user_tz":-480},"id":"hMDBXj7Hg-0e","outputId":"c9d59d4d-fa46-41a0-a68a-fd81782313ed"},"outputs":[],"source":["# sentiment_SELF\n","self_file_path = \"data/emojitag_remove_filtered_2278.csv\"\n","self_df = pd.read_csv(self_file_path)\n","x_self_test = pad_sequences(tokenizer.texts_to_sequences(self_df[\"processed_text\"]), maxlen=SEQUENCE_LENGTH)\n","\n","y_self_test = encoder.transform(self_df[\"sentiment_SELF\"].tolist())\n","y_self_test_num = y_self_test.reshape(-1, 1)\n","y_self_test = np_utils.to_categorical(y_self_test_num)\n","\n","loss, accuracy = model_lstm.evaluate(x_self_test, y_self_test, batch_size=BATCH_SIZE, verbose=1)\n","print(\"\\nSelf-labeled Test Accuracy = {}\".format(accuracy))\n","self_df.head(1)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":174622,"status":"ok","timestamp":1618125375950,"user":{"displayName":"Tianyu Fu","photoUrl":"","userId":"03175330516896294618"},"user_tz":-480},"id":"_GO5diIZnlK4","outputId":"4e0eb840-6889-484a-b285-c81a4a9d2a63"},"outputs":[],"source":["def v2sentiment(list):\n","    sentiments = []\n","    for v in list:\n","        if v == 2: sentiments.append(\"positive\")\n","        elif v == 1: sentiments.append(\"neutral\")\n","        elif v == 0: sentiments.append(\"negative\")\n","    return sentiments\n","    \n","y_self_pred = np.argmax(model_lstm.predict(x_self_test), axis=-1)\n","self_df[\"sentiment_tagged\"] = v2sentiment(y_self_pred)\n","self_df.to_csv(self_file_path, index=False)\n","\n","accuracy = accuracy_score(y_self_test_num, y_self_pred)\n","print(\"Self-labeled Test Accuracy = {}\".format(accuracy))\n","\n","f1 = f1_score(y_self_test_num, y_self_pred, average=\"macro\")\n","print(\"Self-labeled Test F1 macro score = {}\".format(f1))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"H06Uub9SutPj"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["psFgH5Qz7bxB","hF3EF4ec7i76"],"name":"SA_tag_LSTM.ipynb","provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.3"}},"nbformat":4,"nbformat_minor":0}
