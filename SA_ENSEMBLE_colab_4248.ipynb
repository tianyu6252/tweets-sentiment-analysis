{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.7"},"colab":{"name":"SentimentAnalysis_ENSEMBLE_colab_4248.ipynb","provenance":[],"collapsed_sections":["fp2sYFRGMAYt","1Zrm-sI2MAYu"],"toc_visible":true}},"cells":[{"cell_type":"markdown","metadata":{"id":"2k4ykxTFMAYo"},"source":["# Sentiment Analysis ENSEMBLE"]},{"cell_type":"markdown","metadata":{"id":"o-vlPpi8MAYp"},"source":["# Import Libraries"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YVVzia33MAYq","outputId":"315800e1-3063-4a70-e898-cb9f3faff19a"},"source":["import pandas as pd\n","pd.set_option('display.max_colwidth', None)\n","# pd.set_option('display.max_rows', None)\n","import numpy as np\n","from numpy import load\n","import pickle\n","\n","import tensorflow\n","from tensorflow.keras.preprocessing import text, sequence\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Activation, Conv1D, Dense, Dropout, Embedding, InputLayer, Flatten, GlobalMaxPool1D, LSTM, MaxPooling1D, Dropout\n","from tensorflow.keras import regularizers\n","from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras import backend as K\n","\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n","from sklearn.feature_extraction.text import CountVectorizer\n","\n","import gensim\n","from gensim.models import Word2Vec, KeyedVectors\n","\n","import re\n","import string\n","from string import punctuation\n","import seaborn as sb\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","import time\n","import statistics\n","\n","# Open-source Sentiment Analysis libraries\n","from textblob import TextBlob\n","import nltk\n","from nltk.sentiment import SentimentIntensityAnalyzer\n","nltk.download('vader_lexicon')\n","\n","# Self-created python file\n","!pip install emoji # Note preprocessing_steps has emoji as dependent library\n","!pip install ekphrasis\n","from preprocessing_steps import Preprocess\n","\n","# To read/write into Google drive file by Share URL keys\n","from io import BytesIO\n","from google.colab import auth, files, drive\n","drive.mount('/content/gdrive')\n","auth.authenticate_user()\n","from oauth2client.client import GoogleCredentials\n","from apiclient.http import MediaFileUpload\n","from googleapiclient.discovery import build\n","\n","# Get auth credentials and service\n","creds = GoogleCredentials.get_application_default()\n","service = build('drive', 'v3', credentials=creds)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n","[nltk_data]   Package vader_lexicon is already up-to-date!\n","Requirement already satisfied: emoji in /usr/local/lib/python3.7/dist-packages (1.2.0)\n","Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"6fge6KVvXTuE"},"source":["def save_dataframe_to_csv_results_by_sharedurlkey(data_df, csv_file_id):\n","    # Save a temp copy into personal Google drive\n","    csv_file_name = 'TEMP_file.csv'\n","    csv_file_path = f\"/content/gdrive/MyDrive/{csv_file_name}\"\n","    data_df.to_csv(csv_file_path, index=False)\n","\n","    # Create media_body from csv file that was saved into personal Google drive\n","    # Then upload/update back to existing csv file with shared url key\n","    csv_media_body = MediaFileUpload(csv_file_path, resumable=True) \n","    service.files().update(\n","        fileId = csv_file_id,\n","        media_body = csv_media_body\n","        ).execute()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"U_iOXUfOMAYq"},"source":["# Read and Preprocess Data"]},{"cell_type":"code","metadata":{"scrolled":true,"id":"nQ-WuGRgMAYq"},"source":["# data_df = pd.read_csv('tweets_compiled_2020_06_to_2021_02.csv', encoding='utf-8')\n","# data_df = pd.read_csv('tweets_filtered_compiled_2020_06_to_2021_02.csv', encoding='utf-8')\n","# data_df = pd.read_csv('tweets_compiled_2020_09_to_2021_02.csv', encoding='utf-8')\n","data_df = pd.read_csv('tweets_filtered_compiled_2020_09_to_2021_02.csv', encoding='utf-8')\n","print(len(data_df))\n","data_df.head(10)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"z8b8wOZcMAYs"},"source":["# Preprocess Text (Only need to do this step if not yet preprocessed)"]},{"cell_type":"code","metadata":{"id":"-_ikASRZMAYs","colab":{"base_uri":"https://localhost:8080/"},"outputId":"7308e523-7c1a-4b77-ffa2-7f2494f73449"},"source":["def process_text(text):\n","    # text = Preprocess.replace_unrecognised_open_close_inverted_comma(text) # Replaces â€œ â€ â€˜ â€™ with \" \" ' '\n","    text = Preprocess.remove_urls(text)\n","    text = Preprocess.remove_digits(text)\n","\n","    # EMOJI/EMOTICONS handling\n","    text = Preprocess.convert_emojis_to_sentiments(text)\n","    text = Preprocess.remove_emojis(text)\n","    text = Preprocess.convert_emoticons_to_sentiments(text)   \n","    # text = Preprocess.convert_emojis_to_words(text) # Note Sentiment140 dataset has no emojis\n","    # text = Preprocess.replace_hyphens_or_underscores_with_spaces(text)\n","\n","    # Entity Recognition handling\n","    text = Preprocess.entity_recognition_handling(text) # NOTE: Takes consideration amount of time for large datasets\n","\n","    # Convert all text to lowercase (Note: Have to perform it after entity recognition handling)\n","    text = Preprocess.convert_to_lowercase(text)\n","\n","    # SLANGS handling\n","    text = Preprocess.convert_slangs_to_words(text)\n","\n","    # NEGATION handling\n","    text = Preprocess.prepend_NOT_to_handle_negation(text)\n","    text = Preprocess.remove_stopwords_after_appended_NOT(text)\n","\n","    # Handle hashtags with concatenated words\n","    # text = Preprocess.perform_word_segmentation_for_hashtags(text)\n","\n","    # Handle incorrectly spelt words\n","    # text = Preprocess.correct_spelling_mistakes(text) # NOTE: Takes consideration amount of time for large datasets\n","\n","    # LEMMATIZATION\n","    text = Preprocess.perform_lemmatization(text)\n","    \n","    # Handle elongated words\n","    # text = Preprocess.handle_elongated_words(text)\n","\n","    # Remove punctuations and special characters except hyphers and underscores that identifies entities\n","    text = Preprocess.remove_special_characters_except_hyphens_and_underscores(text)\n","    \n","    # Remove words with very short char length (e.g. length 2)\n","    text = Preprocess.remove_characters_of_specified_length(text, 2)\n","\n","    return text\n","\n","start = time.time()\n","data_df[\"processed_text\"] = data_df[\"text\"].apply(process_text)\n","# data_df[\"processed_text\"] = data_df[\"full_text\"].apply(Preprocess.process_text)\n","print(\"\\nPreprocessing of Text COMPLETED\")\n","print(\"Total time taken in minutes: {:.4f}\".format((time.time()-start) / 60))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\n","Total time taken in minutes: 11.2345\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"8gmGaEMJMAYt"},"source":["# Make a DataFrame Copy (Just in case)"]},{"cell_type":"code","metadata":{"id":"fX4e5ViFMAYt"},"source":["data_copy = data_df.copy()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fp2sYFRGMAYt"},"source":["# TEXTBLOB SA"]},{"cell_type":"code","metadata":{"id":"Z-ZUHd26MAYu","colab":{"base_uri":"https://localhost:8080/"},"outputId":"305b38a0-430f-4803-c7e8-25952db08c66"},"source":["start = time.time()\n","sentiment_TextBlob = []\n","for i in range(len(data_copy)):\n","    blob = TextBlob(data_copy.iloc[i][\"processed_text\"]) # Note: On processed_text\n","    sentiment = \"neutral\"\n","    if blob.sentiment.polarity == 0:\n","        sentiment = \"neutral\"\n","    elif blob.sentiment.polarity > 0:\n","        sentiment = \"positive\"\n","    elif blob.sentiment.polarity < 0:\n","        sentiment = \"negative\"\n","        \n","    sentiment_TextBlob.append(sentiment)\n","\n","# Add in new column called \"sentiment_TextBlob\" with the sentiment value from TextBlob\n","data_copy[\"sentiment_TextBlob\"] = sentiment_TextBlob\n","print(str(len(data_copy)) + \" records\")\n","print(\"\\nTotal time taken in minutes: {:.4f}\".format((time.time()-start) / 60))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["48948 records\n","\n","Total time taken in minutes: 0.3567\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"1Zrm-sI2MAYu"},"source":["# NLTK VADER SA\n","Valence Aware Dictionary and sEntiment Reasoner"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1t_JIPgvMAYv","outputId":"fedc17c8-b4b2-41f5-8f7d-58eec2446086"},"source":["sia = SentimentIntensityAnalyzer()\n","\n","start = time.time()\n","sentiment_Vader = []\n","for i in range(len(data_copy)):\n","    # scores_dict = sia.polarity_scores(data_copy.iloc[i][\"text\"]) # Note: On unprocessed text because it has an exhaustive inbuilt preprocessing\n","    scores_dict = sia.polarity_scores(data_copy.iloc[i][\"full_text\"]) # Note: On unprocessed text because it has an exhaustive inbuilt preprocessing\n","    sentiment = \"neutral\"\n","    if scores_dict[\"compound\"] == 0:\n","        sentiment = \"neutral\"\n","    elif scores_dict[\"compound\"] > 0:\n","        sentiment = \"positive\"\n","    elif scores_dict[\"compound\"] < 0:\n","        sentiment = \"negative\"\n","        \n","    sentiment_Vader.append(sentiment)\n","\n","# Add in new column called \"sentiment_TewxtBlob\" with the sentiment value from TextBlob\n","data_copy[\"sentiment_Vader\"] = sentiment_Vader\n","print(str(len(data_copy)) + \" records\")\n","print(\"\\nTotal time taken in minutes: {:.4f}\".format((time.time()-start) / 60))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["48948 records\n","\n","Total time taken in minutes: 0.3976\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Qi68H6sjMAYy"},"source":["# LSTM SA"]},{"cell_type":"markdown","metadata":{"id":"7yLZ8DVv5LFY"},"source":["## Load saved processed embedding matrix (GloVe twitter 27b 200d)"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Tphd8WWT5SBe","outputId":"0f25ab54-f02e-46db-ff9d-6e7d6d28f865"},"source":["# Load embedding matrix file by Shared URL key\n","# https://drive.google.com/file/d/1740LvMrdxGEKFlobMW-4ZuGOzcdh5zlI/view?usp=sharing - V3\n","embedding_matrix_file_id = \"1740LvMrdxGEKFlobMW-4ZuGOzcdh5zlI\"\n","embedding_matrix_file = service.files().get_media(fileId=embedding_matrix_file_id).execute()\n","\n","# Load EMBEDDING_MATRIX(numpy array) from npy file\n","EMBEDDING_MATRIX = load(BytesIO(embedding_matrix_file))\n","print(\"EMBEDDING_MATRIX SHAPE:\", EMBEDDING_MATRIX.shape)\n","\n","VOCAB_SIZE = EMBEDDING_MATRIX.shape[0]\n","EMBED_DIM = EMBEDDING_MATRIX.shape[1]\n","MAX_SEQUENCE_LENGTH = EMBEDDING_MATRIX.shape[1]\n","\n","print(\"VOCAB_SIZE - Embedding Matrix:\", VOCAB_SIZE)\n","print(\"EMBED_DIM: \", EMBED_DIM)\n","print(\"MAX_SEQUENCE_LENGTH: \", MAX_SEQUENCE_LENGTH)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["(484294, 200)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"aFbHk_Sq9doA"},"source":["## Load pretrained LSTM weights"]},{"cell_type":"code","metadata":{"id":"fvhqSzGK9bc8"},"source":["# Model function to construct model\n","def get_LSTM_Model(embedding_layer):\n","    model = Sequential()\n","    model.add(InputLayer(input_shape=(MAX_SEQUENCE_LENGTH,), dtype='int32'))\n","    \n","    # Non-trainable embedding layer\n","    model.add(embedding_layer)\n","    \n","    # LSTM layer\n","    model.add(LSTM(128, return_sequences=True))\n","    model.add(GlobalMaxPool1D())\n","    model.add(Dropout(0.1))\n","    model.add(Dense(64, activation='relu'))\n","    model.add(Dropout(0.1))\n","    model.add(Dense(32, activation='relu'))\n","    model.add(Dropout(0.1))\n","    model.add(Dense(1, activation='sigmoid'))\n","\n","    model.compile(loss='binary_crossentropy',\n","                  optimizer='adam',\n","                  metrics=['accuracy'])\n","    return model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EOWI9niyVDal"},"source":["# Load embedding layer and load trained lstm weights\n","embedding_layer = Embedding(VOCAB_SIZE,\n","                            EMBED_DIM,\n","                            input_length=MAX_SEQUENCE_LENGTH,\n","                            weights=[EMBEDDING_MATRIX],\n","                            trainable=False)\n","model = get_LSTM_Model(embedding_layer)\n","lstm_model_file_path = \"lstm_main_model_V3.h5\"\n","model.load_weights(lstm_model_file_path) # Can't seem to properly load_weights by file. Can only provide direct filepath link to file"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"s9MXXFGiFOTa"},"source":["## Load Tokenizer and define utility functions for model"]},{"cell_type":"code","metadata":{"id":"AN4XS_jJ7xfY"},"source":["# Load tokenizer file by Shared URL key\n","# https://drive.google.com/file/d/106RIuB05hSHQtYYjTV4szFSa5EhlvoaS/view?usp=sharing - V3\n","tokenizer_file_id = \"106RIuB05hSHQtYYjTV4szFSa5EhlvoaS\"\n","tokenizer_file = service.files().get_media(fileId=tokenizer_file_id).execute()\n","tokenizer = pickle.load(BytesIO(tokenizer_file))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jJYzdu4AVQee"},"source":["# Define utility models to test the model\n","def decode_sentiment(score, include_neutral=True):\n","    SENTIMENT_THRESHOLDS = (0.4, 0.6) # \"neutral\" will be in between 0.4 and 0.6\n","    if include_neutral:        \n","        label = \"neutral\"\n","        if score <= SENTIMENT_THRESHOLDS[0]:\n","            label = \"negative\"\n","        elif score >= SENTIMENT_THRESHOLDS[1]:\n","            label = \"positive\"\n","        return label\n","    else:\n","        return \"negative\" if score < 0.5 else \"positive\"\n","\n","def predict(model, text, include_neutral=True): # Original/Base/V3 version\n","    # Tokenize text\n","    x_test = pad_sequences(tokenizer.texts_to_sequences([text]), maxlen=MAX_SEQUENCE_LENGTH)\n","    # Predict\n","    score = model.predict([x_test])[0] # score is a value from 0 to 1 (e.g. 0.56)\n","    # Decode sentiment\n","    label = decode_sentiment(score, include_neutral=include_neutral)\n","    return {\"label\": label, \"score\": float(score)}  \n","    \n","# def predict(model, text, include_neutral=True): # V2 version (softmax. 1hot-encoded labels)\n","#     # Tokenize text\n","#     # x_test = pad_sequences(tokenizer.texts_to_sequences([text]), maxlen=MAX_SEQUENCE_LENGTH)\n","#     x_test = pad_sequences(tokenizer.texts_to_sequences([text]), padding='post',  maxlen=MAX_SEQUENCE_LENGTH) # V2\n","#     # Predict (Note: This method can only have positive or negative)\n","#     label = \"negative\"\n","#     label_val = np.argmax(model.predict(x_test), axis=-1) # Predict 0 or 1 (0=negative, 1=positive)\n","#     if label_val == 1:\n","#         label = \"positive\"\n","#     return {\"label\": label}  "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2EhZCx859OIE","outputId":"d17d71aa-cd89-4fe9-d51d-0b794a0f47a1"},"source":["predict(model, \"i love the music\")"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'label': 'positive', 'score': 0.9586430788040161}"]},"metadata":{"tags":[]},"execution_count":15}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"S0C9SAbf7_ne","outputId":"ad2f1406-74b9-4c61-c9d4-f0df846cf552"},"source":["predict(model, \"i hate the rain\")"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'label': 'negative', 'score': 0.02842596173286438}"]},"metadata":{"tags":[]},"execution_count":16}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"k_M6SGUaSR_p","outputId":"5251a359-dd72-4d19-bedf-a737f0b57676"},"source":["orig_text = data_copy.iloc[0][\"text\"]\n","sample_text = data_copy.iloc[0][\"processed_text\"]\n","print(\"orig_text: \" + orig_text)\n","print(predict(model, orig_text))\n","print(\"------------------------------------------------------\")\n","print(\"\\nsample_text: \" + sample_text)\n","print(predict(model, sample_text))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["orig_text: We have a list of 200 kids that signed up for the backpack giveaway. Iâ€™m in need of hand sanitizer to include. If youâ€™re willing to donate you can receive a receipt ðŸ§¾ for a tax deduction. I need by Friday Septemberâ€¦ https://t.co/9e6LQ1h0nx\n","\n","sample_text: list kid signed backpack giveaway need hand sanitizer include willing donate receive receipt receipt tax deduction need friday_september\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["{'label': 'positive', 'score': 0.8696259260177612}"]},"metadata":{"tags":[]},"execution_count":17}]},{"cell_type":"markdown","metadata":{"id":"ctKpSH9pTWMK"},"source":["## Perform LSTM SA Predictions on Data"]},{"cell_type":"code","metadata":{"id":"8zfblL4xMAY0","colab":{"base_uri":"https://localhost:8080/"},"outputId":"b5d10883-7e75-4624-9df4-507736be651c"},"source":["import time\n","\n","start = time.time()\n","sentiment_LSTM = []\n","for i in range(len(data_copy)):\n","    result = predict(model, data_copy.iloc[i][\"processed_text\"])\n","    sentiment_LSTM.append(result[\"label\"].lower())\n","        \n","# Add in new column called \"sentiment_LSTM\" with the sentiment value from trained LSTM model\n","data_copy[\"sentiment_LSTM\"] = sentiment_LSTM\n","\n","print(str(len(data_copy)) + \" records\")\n","print(\"\\nTotal time taken in minutes: {:.4f}\".format((time.time()-start) / 60))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["48948 records\n","\n","Total time taken in minutes: 40.9246\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"MgBoP-d7TySV"},"source":["# Perform MAX-VOTING SA based on all the classifiers"]},{"cell_type":"code","metadata":{"id":"rvDqyNdhMAY1","colab":{"base_uri":"https://localhost:8080/"},"outputId":"0913fb55-c1b7-453f-b477-822450674c02"},"source":["start = time.time()\n","\n","data_temp = data_copy.copy()\n","columns = [\"sentiment_TextBlob\", \"sentiment_Vader\", \"sentiment_LSTM\", \"sentiment_Google\"]\n","data_temp = data_temp[columns]\n","# Update new column \"sentiment_MAX_VOTE\"\n","data_copy[\"MAX_VOTE\"] = data_temp.mode(axis='columns')[0]\n","\n","print(\"\\nTotal time taken in minutes: {:.4f}\".format((time.time()-start) / 60))\n","\n","data_copy.head(3)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\n","Total time taken in minutes: 0.2991\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":593},"id":"gMD3bYJ1VGCw","outputId":"8bf18d6f-c4ff-43ff-c79d-04a90db55e58"},"source":["data_copy.head(1)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>created_at</th>\n","      <th>id</th>\n","      <th>id_str</th>\n","      <th>full_text</th>\n","      <th>truncated</th>\n","      <th>display_text_range</th>\n","      <th>entities</th>\n","      <th>source</th>\n","      <th>in_reply_to_status_id</th>\n","      <th>in_reply_to_status_id_str</th>\n","      <th>in_reply_to_user_id</th>\n","      <th>in_reply_to_user_id_str</th>\n","      <th>in_reply_to_screen_name</th>\n","      <th>user</th>\n","      <th>geo</th>\n","      <th>coordinates</th>\n","      <th>place</th>\n","      <th>contributors</th>\n","      <th>is_quote_status</th>\n","      <th>retweet_count</th>\n","      <th>favorite_count</th>\n","      <th>favorited</th>\n","      <th>retweeted</th>\n","      <th>possibly_sensitive</th>\n","      <th>lang</th>\n","      <th>extended_entities</th>\n","      <th>quoted_status_id</th>\n","      <th>quoted_status_id_str</th>\n","      <th>quoted_status_permalink</th>\n","      <th>quoted_status</th>\n","      <th>country</th>\n","      <th>processed_text</th>\n","      <th>sentiment_TextBlob</th>\n","      <th>sentiment_Vader</th>\n","      <th>sentiment_LSTM</th>\n","      <th>sentiment_MAX_VOTE</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>2020-09-01 04:25:47+00:00</td>\n","      <td>1300651040526733314</td>\n","      <td>1300651040526733312</td>\n","      <td>We have a list of 200 kids that signed up for the backpack giveaway. Iâ€™m in need of hand sanitizer to include. If youâ€™re willing to donate you can receive a receipt ðŸ§¾ for a tax deduction. I need by Friday Septemberâ€¦ https://t.co/9e6LQ1h0nx</td>\n","      <td>False</td>\n","      <td>[0, 239]</td>\n","      <td>{'hashtags': [], 'symbols': [], 'user_mentions': [], 'urls': [{'url': 'https://t.co/9e6LQ1h0nx', 'expanded_url': 'https://www.instagram.com/p/CElJeg4s3db/?igshid=1jjktp8i9ifji', 'display_url': 'instagram.com/p/CElJeg4s3db/â€¦', 'indices': [216, 239]}]}</td>\n","      <td>&lt;a href=\"http://instagram.com\" rel=\"nofollow\"&gt;Instagram&lt;/a&gt;</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>{'id': 191658725, 'id_str': '191658725', 'name': '1st Lady Lisa', 'screen_name': '1stLadyLisa1', 'location': 'Sacramento', 'description': \"CEO of FANS Radio 916 ðŸ“»ðŸŽ¶ðŸŽ¶ Listen https://t.co/J8GIkpsppw &amp; Saturday's from 10pm - 12am on KUBU 96.5 https://t.co/v86AQVfbGN\", 'url': 'https://t.co/lqznIqgv9e', 'entities': {'url': {'urls': [{'url': 'https://t.co/lqznIqgv9e', 'expanded_url': 'http://www.numberonemusic.com/1stLadyLisa', 'display_url': 'numberonemusic.com/1stLadyLisa', 'indices': [0, 23]}]}, 'description': {'urls': [{'url': 'https://t.co/J8GIkpsppw', 'expanded_url': 'http://www.saucedupradio.com', 'display_url': 'saucedupradio.com', 'indices': [33, 56]}, {'url': 'https://t.co/v86AQVfbGN', 'expanded_url': 'http://www.accesssacramento.org', 'display_url': 'accesssacramento.org', 'indices': [100, 123]}]}}, 'protected': False, 'followers_count': 1923, 'friends_count': 1726, 'listed_count': 41, 'created_at': 'Fri Sep 17 00:43:01 +0000 2010', 'favourites_count': 4640, 'utc_offset': None, 'time_zone': None, 'geo_enabled': True, 'verified': False, 'statuses_count': 27123, 'lang': None, 'contributors_enabled': False, 'is_translator': False, 'is_translation_enabled': False, 'profile_background_color': '642D8B', 'profile_background_image_url': 'http://abs.twimg.com/images/themes/theme10/bg.gif', 'profile_background_image_url_https': 'https://abs.twimg.com/images/themes/theme10/bg.gif', 'profile_background_tile': False, 'profile_image_url': 'http://pbs.twimg.com/profile_images/464632798062460928/GJMGdgbr_normal.jpeg', 'profile_image_url_https': 'https://pbs.twimg.com/profile_images/464632798062460928/GJMGdgbr_normal.jpeg', 'profile_banner_url': 'https://pbs.twimg.com/profile_banners/191658725/1478454203', 'profile_image_extensions_alt_text': None, 'profile_banner_extensions_alt_text': None, 'profile_link_color': 'FF0000', 'profile_sidebar_border_color': 'FFFFFF', 'profile_sidebar_fill_color': '7AC3EE', 'profile_text_color': '3D1957', 'profile_use_background_image': True, 'has_extended_profile': False, 'default_profile': False, 'default_profile_image': False, 'following': False, 'follow_request_sent': False, 'notifications': False, 'translator_type': 'none'}</td>\n","      <td>{'type': 'Point', 'coordinates': [38.577, -121.4947]}</td>\n","      <td>{'type': 'Point', 'coordinates': [-121.4947, 38.577]}</td>\n","      <td>{'id': 'b71fac2ee9792cbe', 'url': 'https://api.twitter.com/1.1/geo/id/b71fac2ee9792cbe.json', 'place_type': 'city', 'name': 'Sacramento', 'full_name': 'Sacramento, CA', 'country_code': 'US', 'country': 'United States', 'contained_within': [], 'bounding_box': {'type': 'Polygon', 'coordinates': [[[-121.576613, 38.43792], [-121.362715, 38.43792], [-121.362715, 38.6855236], [-121.576613, 38.6855236]]]}, 'attributes': {}}</td>\n","      <td>NaN</td>\n","      <td>False</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>0.0</td>\n","      <td>en</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>United States</td>\n","      <td>list kid signed backpack giveaway need hand sanitizer include willing donate receive receipt receipt tax deduction need friday_september</td>\n","      <td>positive</td>\n","      <td>positive</td>\n","      <td>positive</td>\n","      <td>positive</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                  created_at  ...  sentiment_MAX_VOTE\n","0  2020-09-01 04:25:47+00:00  ...            positive\n","\n","[1 rows x 36 columns]"]},"metadata":{"tags":[]},"execution_count":24}]},{"cell_type":"markdown","metadata":{"id":"7mbERiAlVIBt"},"source":["# Save Final Results into Google Drive main file"]},{"cell_type":"code","metadata":{"id":"anXywokNMAY1"},"source":["# save_dataframe_to_csv_results_by_sharedurlkey(data_copy, csv_file_id)"],"execution_count":null,"outputs":[]}]}